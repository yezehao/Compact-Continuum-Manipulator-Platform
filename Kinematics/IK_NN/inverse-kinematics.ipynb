{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Module Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as dataprocess\n",
    "\n",
    "# Auxiliary Module Import\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU Check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: angle, Size: (100000000, 4)\n",
      "Parameter name: position, Size: (100000000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load MATLAB v7.3 File\n",
    "mat_file = h5py.File('(UT)_dataset/Angle_v_Position_100000000.mat', 'r')\n",
    "\n",
    "# Dataset Check\n",
    "keys = list(mat_file.keys())\n",
    "for key in keys:\n",
    "    dataset = mat_file[key]\n",
    "    print(f\"Parameter name: {key}, Size: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The angle dataset size: torch.Size([100000000, 4]) The device: cuda:0\n",
      "The position dataset size: torch.Size([100000000, 3]) The device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create Input and Output Tensor\n",
    "angle = torch.tensor(mat_file['angle'][:], dtype=torch.float32).to(device)\n",
    "position = torch.tensor(mat_file['position'][:], dtype=torch.float32).to(device)\n",
    "print(f\"The angle dataset size: {angle.size()} The device: {angle.device}\")\n",
    "print(f\"The position dataset size: {position.size()} The device: {position.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 40960\n",
      "Batch Position Size: torch.Size([512, 4])\n",
      "Batch Angle Size: torch.Size([512, 4])\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "batch_size = 512  # the batch size can be changed from 1 to 4096\n",
    "epochs = 200 # the number of training epochs is 200\n",
    "subset = 40960 # the sub dataset number\n",
    "step_size = 10 # change the learning rate every 10 epochs\n",
    "gamma = 0.7 # learning rate schedule ratio\n",
    "lr = 0.2 # initial learning rate\n",
    "\n",
    "# Dataset Initialzation\n",
    "subset_indices = torch.randperm(len(dataset))[:subset] # generate sub dataset indices\n",
    "subset_position = position[subset_indices] # select sub_dataset of position from dataset of position \n",
    "subset_angle = angle[subset_indices] # select sub_dataset of angle from dataset of angle\n",
    "input_data = torch.cat([torch.ones(subset_position.size(0), 1, device=device), subset_position], dim=1)\n",
    "output_data = subset_angle \n",
    "dataset = dataprocess.TensorDataset(input_data, output_data)\n",
    "dataloader = dataprocess.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset Size: {len(dataset)}\")\n",
    "for batch_position, batch_angle in dataloader:\n",
    "    print(\"Batch Position Size:\", batch_position.size())\n",
    "    print(\"Batch Angle Size:\", batch_angle.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The activation function\n",
    "There are multiple activation functions to use in the ANN. The most commonly used is the ReLU (Rectified Linear Unit), which is quit simple. However, this activation function may have some issues in certain points. In hence, we need to use other activation funtion like Leaky ReLU `torch.nn.LeakyReLU` and GELU `torch.nn.functional.gelu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ANN Model Construction\n",
    "# class CustomANN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomANN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(4, 5)  # input layer to hidden layer\n",
    "#         self.fc2 = nn.Linear(5, 4)  # hidden layer to output layer\n",
    "#         self.activation = nn.ReLU() # use ReLU as activation function\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Model Initialization\n",
    "# model = CustomANN() # create an instance\n",
    "# model.to(device) # deploy model to cuda\n",
    "# criterion = nn.MSELoss() # define loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr = lr) # define optimizer\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)  # learning rate schedule\n",
    "\n",
    "# # Initialize an empty list to store the losses\n",
    "# losses = []\n",
    "\n",
    "# # Model Training\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_losses = []  # losses for the current epoch\n",
    "#     for batch_position, batch_angle in dataloader:\n",
    "#         # Forward Propagation\n",
    "#         outputs = model(input_data)\n",
    "#         loss = criterion(outputs, output_data)\n",
    "\n",
    "#         # Backward Propagation and Optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#         epoch_losses.append(loss.item())\n",
    "\n",
    "#     # Calculate the average loss for the epoch\n",
    "#     average_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "#     losses.append(average_epoch_loss)  # append the average loss to the list\n",
    "\n",
    "#     # Learning Rate Scheduler \n",
    "#     scheduler.step()\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "# # Plot the loss\n",
    "# plt.plot(losses)\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "\n",
    "# # Model Saving\n",
    "# torch.save(model.state_dict(), 'model/custom_ReLU_ann_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ANN Model Construction\n",
    "# class CustomANN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomANN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(4, 5)  # input layer to hidden layer\n",
    "#         self.fc2 = nn.Linear(5, 4)  # hidden layer to output layer\n",
    "#         self.activation = nn.LeakyReLU() # use ReLU as activation function\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Model Initialization\n",
    "# model = CustomANN() # create an instance\n",
    "# model.to(device) # deploy model to cuda\n",
    "# criterion = nn.MSELoss() # define loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr = lr) # define optimizer\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)  # learning rate schedule\n",
    "\n",
    "# # Initialize an empty list to store the losses\n",
    "# losses = []\n",
    "\n",
    "# # Model Training\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_losses = []  # losses for the current epoch\n",
    "#     for batch_position, batch_angle in dataloader:\n",
    "#         # Forward Propagation\n",
    "#         outputs = model(input_data)\n",
    "#         loss = criterion(outputs, output_data)\n",
    "\n",
    "#         # Backward Propagation and Optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_losses.append(loss.item())\n",
    "\n",
    "#     # Calculate the average loss for the epoch\n",
    "#     average_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "#     losses.append(average_epoch_loss)  # append the average loss to the list\n",
    "    \n",
    "#     # Learning Rate Scheduler \n",
    "#     scheduler.step()\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "# # Plot the loss\n",
    "# plt.plot(losses)\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "\n",
    "# # Model Saving\n",
    "# torch.save(model.state_dict(), 'model/custom_LeakyReLU_ann_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GELU (Gaussian-error Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ANN Model Construction\n",
    "# class CustomANN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomANN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(4, 5)  # input layer to hidden layer\n",
    "#         self.fc2 = nn.Linear(5, 4)  # hidden layer to output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = nn.functional.gelu(self.fc1(x))  # Applying GELU activation\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Model Initialization\n",
    "# model = CustomANN() # create an instance\n",
    "# model.to(device) # deploy model to cuda\n",
    "# criterion = nn.MSELoss() # define loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr = lr) # define optimizer\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)  # learning rate schedule\n",
    "\n",
    "# # Initialize an empty list to store the losses\n",
    "# losses = []\n",
    "\n",
    "# # Model Training\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_losses = []  # losses for the current epoch\n",
    "#     for batch_position, batch_angle in dataloader:\n",
    "#         # Forward Propagation\n",
    "#         outputs = model(input_data)\n",
    "#         loss = criterion(outputs, output_data)\n",
    "\n",
    "#         # Backward Propagation and Optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#         epoch_losses.append(loss.item())\n",
    "\n",
    "#     # Calculate the average loss for the epoch\n",
    "#     average_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "#     losses.append(average_epoch_loss)  # append the average loss to the list\n",
    "    \n",
    "#     # Learning Rate Scheduler \n",
    "#     scheduler.step()\n",
    "#     print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "# # Plot the loss\n",
    "# plt.plot(losses)\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "\n",
    "# # Model Saving\n",
    "# torch.save(model.state_dict(), 'model/custom_GeLU_ann_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 4027.1750, Learning Rate: 0.020000\n",
      "Epoch [2/200], Loss: 3978.1375, Learning Rate: 0.020000\n",
      "Epoch [3/200], Loss: 3840.6492, Learning Rate: 0.020000\n",
      "Epoch [4/200], Loss: 3586.1267, Learning Rate: 0.020000\n",
      "Epoch [5/200], Loss: 3339.8594, Learning Rate: 0.020000\n",
      "Epoch [6/200], Loss: 3263.0520, Learning Rate: 0.020000\n",
      "Epoch [7/200], Loss: 3227.3452, Learning Rate: 0.020000\n",
      "Epoch [8/200], Loss: 3218.1951, Learning Rate: 0.020000\n",
      "Epoch [9/200], Loss: 3210.3662, Learning Rate: 0.020000\n",
      "Epoch [10/200], Loss: 3195.9978, Learning Rate: 0.018000\n",
      "Epoch [11/200], Loss: 3185.7415, Learning Rate: 0.018000\n",
      "Epoch [12/200], Loss: 3180.7202, Learning Rate: 0.018000\n",
      "Epoch [13/200], Loss: 3172.3040, Learning Rate: 0.018000\n",
      "Epoch [14/200], Loss: 3175.0452, Learning Rate: 0.018000\n",
      "Epoch [15/200], Loss: 3167.3208, Learning Rate: 0.018000\n",
      "Epoch [16/200], Loss: 3164.9717, Learning Rate: 0.018000\n",
      "Epoch [17/200], Loss: 3163.6238, Learning Rate: 0.018000\n",
      "Epoch [18/200], Loss: 3166.0652, Learning Rate: 0.018000\n",
      "Epoch [19/200], Loss: 3164.5442, Learning Rate: 0.018000\n",
      "Epoch [20/200], Loss: 3157.8667, Learning Rate: 0.016200\n",
      "Epoch [21/200], Loss: 3154.3105, Learning Rate: 0.016200\n",
      "Epoch [22/200], Loss: 3158.1746, Learning Rate: 0.016200\n",
      "Epoch [23/200], Loss: 3158.3123, Learning Rate: 0.016200\n",
      "Epoch [24/200], Loss: 3158.0164, Learning Rate: 0.016200\n",
      "Epoch [25/200], Loss: 3158.0610, Learning Rate: 0.016200\n",
      "Epoch [26/200], Loss: 3158.4470, Learning Rate: 0.016200\n",
      "Epoch [27/200], Loss: 3152.7119, Learning Rate: 0.016200\n",
      "Epoch [28/200], Loss: 3157.1328, Learning Rate: 0.016200\n",
      "Epoch [29/200], Loss: 3151.5798, Learning Rate: 0.016200\n",
      "Epoch [30/200], Loss: 3148.5789, Learning Rate: 0.014580\n",
      "Epoch [31/200], Loss: 3145.5344, Learning Rate: 0.014580\n",
      "Epoch [32/200], Loss: 3147.2527, Learning Rate: 0.014580\n",
      "Epoch [33/200], Loss: 3146.8599, Learning Rate: 0.014580\n",
      "Epoch [34/200], Loss: 3146.2734, Learning Rate: 0.014580\n",
      "Epoch [35/200], Loss: 3148.4929, Learning Rate: 0.014580\n",
      "Epoch [36/200], Loss: 3146.6072, Learning Rate: 0.014580\n",
      "Epoch [37/200], Loss: 3145.0632, Learning Rate: 0.014580\n",
      "Epoch [38/200], Loss: 3149.2371, Learning Rate: 0.014580\n",
      "Epoch [39/200], Loss: 3146.2092, Learning Rate: 0.014580\n",
      "Epoch [40/200], Loss: 3146.2253, Learning Rate: 0.013122\n",
      "Epoch [41/200], Loss: 3142.3862, Learning Rate: 0.013122\n",
      "Epoch [42/200], Loss: 3139.7578, Learning Rate: 0.013122\n",
      "Epoch [43/200], Loss: 3142.3540, Learning Rate: 0.013122\n",
      "Epoch [44/200], Loss: 3140.9465, Learning Rate: 0.013122\n",
      "Epoch [45/200], Loss: 3139.8782, Learning Rate: 0.013122\n",
      "Epoch [46/200], Loss: 3139.1509, Learning Rate: 0.013122\n",
      "Epoch [47/200], Loss: 3138.9958, Learning Rate: 0.013122\n",
      "Epoch [48/200], Loss: 3137.9104, Learning Rate: 0.013122\n",
      "Epoch [49/200], Loss: 3139.5710, Learning Rate: 0.013122\n",
      "Epoch [50/200], Loss: 3140.0486, Learning Rate: 0.011810\n",
      "Epoch [51/200], Loss: 3137.0039, Learning Rate: 0.011810\n",
      "Epoch [52/200], Loss: 3138.0461, Learning Rate: 0.011810\n",
      "Epoch [53/200], Loss: 3139.2930, Learning Rate: 0.011810\n",
      "Epoch [54/200], Loss: 3138.7981, Learning Rate: 0.011810\n",
      "Epoch [55/200], Loss: 3136.2253, Learning Rate: 0.011810\n",
      "Epoch [56/200], Loss: 3139.1753, Learning Rate: 0.011810\n",
      "Epoch [57/200], Loss: 3136.3015, Learning Rate: 0.011810\n",
      "Epoch [58/200], Loss: 3136.3667, Learning Rate: 0.011810\n",
      "Epoch [59/200], Loss: 3133.8782, Learning Rate: 0.011810\n",
      "Epoch [60/200], Loss: 3139.7244, Learning Rate: 0.010629\n",
      "Epoch [61/200], Loss: 3132.9478, Learning Rate: 0.010629\n",
      "Epoch [62/200], Loss: 3133.3201, Learning Rate: 0.010629\n",
      "Epoch [63/200], Loss: 3133.0808, Learning Rate: 0.010629\n",
      "Epoch [64/200], Loss: 3132.5020, Learning Rate: 0.010629\n",
      "Epoch [65/200], Loss: 3132.6560, Learning Rate: 0.010629\n",
      "Epoch [66/200], Loss: 3143.6196, Learning Rate: 0.010629\n",
      "Epoch [67/200], Loss: 3131.2717, Learning Rate: 0.010629\n",
      "Epoch [68/200], Loss: 3134.7312, Learning Rate: 0.010629\n",
      "Epoch [69/200], Loss: 3131.0469, Learning Rate: 0.010629\n",
      "Epoch [70/200], Loss: 3131.0669, Learning Rate: 0.009566\n",
      "Epoch [71/200], Loss: 3129.0671, Learning Rate: 0.009566\n",
      "Epoch [72/200], Loss: 3129.4087, Learning Rate: 0.009566\n",
      "Epoch [73/200], Loss: 3129.6990, Learning Rate: 0.009566\n",
      "Epoch [74/200], Loss: 3128.7310, Learning Rate: 0.009566\n",
      "Epoch [75/200], Loss: 3128.9075, Learning Rate: 0.009566\n",
      "Epoch [76/200], Loss: 3128.7046, Learning Rate: 0.009566\n",
      "Epoch [77/200], Loss: 3128.2766, Learning Rate: 0.009566\n",
      "Epoch [78/200], Loss: 3133.1560, Learning Rate: 0.009566\n",
      "Epoch [79/200], Loss: 3130.3965, Learning Rate: 0.009566\n",
      "Epoch [80/200], Loss: 3128.2505, Learning Rate: 0.008609\n",
      "Epoch [81/200], Loss: 3131.2078, Learning Rate: 0.008609\n",
      "Epoch [82/200], Loss: 3127.6982, Learning Rate: 0.008609\n",
      "Epoch [83/200], Loss: 3128.0154, Learning Rate: 0.008609\n",
      "Epoch [84/200], Loss: 3127.6118, Learning Rate: 0.008609\n",
      "Epoch [85/200], Loss: 3126.6819, Learning Rate: 0.008609\n",
      "Epoch [86/200], Loss: 3127.4197, Learning Rate: 0.008609\n",
      "Epoch [87/200], Loss: 3127.7937, Learning Rate: 0.008609\n",
      "Epoch [88/200], Loss: 3126.6367, Learning Rate: 0.008609\n",
      "Epoch [89/200], Loss: 3127.4851, Learning Rate: 0.008609\n",
      "Epoch [90/200], Loss: 3127.1675, Learning Rate: 0.007748\n",
      "Epoch [91/200], Loss: 3126.1375, Learning Rate: 0.007748\n",
      "Epoch [92/200], Loss: 3127.3774, Learning Rate: 0.007748\n",
      "Epoch [93/200], Loss: 3126.0601, Learning Rate: 0.007748\n",
      "Epoch [94/200], Loss: 3126.1782, Learning Rate: 0.007748\n",
      "Epoch [95/200], Loss: 3126.1118, Learning Rate: 0.007748\n",
      "Epoch [96/200], Loss: 3126.9788, Learning Rate: 0.007748\n",
      "Epoch [97/200], Loss: 3128.5967, Learning Rate: 0.007748\n",
      "Epoch [98/200], Loss: 3125.8833, Learning Rate: 0.007748\n",
      "Epoch [99/200], Loss: 3127.4897, Learning Rate: 0.007748\n",
      "Epoch [100/200], Loss: 3126.9587, Learning Rate: 0.006974\n",
      "Epoch [101/200], Loss: 3125.5029, Learning Rate: 0.006974\n",
      "Epoch [102/200], Loss: 3125.5215, Learning Rate: 0.006974\n",
      "Epoch [103/200], Loss: 3125.4473, Learning Rate: 0.006974\n",
      "Epoch [104/200], Loss: 3125.5779, Learning Rate: 0.006974\n",
      "Epoch [105/200], Loss: 3128.0212, Learning Rate: 0.006974\n",
      "Epoch [106/200], Loss: 3127.8853, Learning Rate: 0.006974\n",
      "Epoch [107/200], Loss: 3126.6379, Learning Rate: 0.006974\n",
      "Epoch [108/200], Loss: 3125.2036, Learning Rate: 0.006974\n",
      "Epoch [109/200], Loss: 3126.4026, Learning Rate: 0.006974\n",
      "Epoch [110/200], Loss: 3125.8362, Learning Rate: 0.006276\n",
      "Epoch [111/200], Loss: 3125.0767, Learning Rate: 0.006276\n",
      "Epoch [112/200], Loss: 3124.9299, Learning Rate: 0.006276\n",
      "Epoch [113/200], Loss: 3126.1567, Learning Rate: 0.006276\n",
      "Epoch [114/200], Loss: 3125.1892, Learning Rate: 0.006276\n",
      "Epoch [115/200], Loss: 3125.2654, Learning Rate: 0.006276\n",
      "Epoch [116/200], Loss: 3125.3096, Learning Rate: 0.006276\n",
      "Epoch [117/200], Loss: 3125.1531, Learning Rate: 0.006276\n",
      "Epoch [118/200], Loss: 3125.3325, Learning Rate: 0.006276\n",
      "Epoch [119/200], Loss: 3125.4988, Learning Rate: 0.006276\n",
      "Epoch [120/200], Loss: 3125.3171, Learning Rate: 0.005649\n",
      "Epoch [121/200], Loss: 3124.6560, Learning Rate: 0.005649\n",
      "Epoch [122/200], Loss: 3124.3806, Learning Rate: 0.005649\n",
      "Epoch [123/200], Loss: 3124.5876, Learning Rate: 0.005649\n",
      "Epoch [124/200], Loss: 3124.5051, Learning Rate: 0.005649\n",
      "Epoch [125/200], Loss: 3124.5574, Learning Rate: 0.005649\n",
      "Epoch [126/200], Loss: 3124.5078, Learning Rate: 0.005649\n",
      "Epoch [127/200], Loss: 3124.7488, Learning Rate: 0.005649\n",
      "Epoch [128/200], Loss: 3125.3396, Learning Rate: 0.005649\n",
      "Epoch [129/200], Loss: 3124.6875, Learning Rate: 0.005649\n",
      "Epoch [130/200], Loss: 3124.3762, Learning Rate: 0.005084\n",
      "Epoch [131/200], Loss: 3124.1738, Learning Rate: 0.005084\n",
      "Epoch [132/200], Loss: 3124.3552, Learning Rate: 0.005084\n",
      "Epoch [133/200], Loss: 3124.4480, Learning Rate: 0.005084\n",
      "Epoch [134/200], Loss: 3124.1833, Learning Rate: 0.005084\n",
      "Epoch [135/200], Loss: 3125.2302, Learning Rate: 0.005084\n",
      "Epoch [136/200], Loss: 3124.5312, Learning Rate: 0.005084\n",
      "Epoch [137/200], Loss: 3125.5935, Learning Rate: 0.005084\n",
      "Epoch [138/200], Loss: 3124.5574, Learning Rate: 0.005084\n",
      "Epoch [139/200], Loss: 3124.3806, Learning Rate: 0.005084\n",
      "Epoch [140/200], Loss: 3123.9590, Learning Rate: 0.004575\n",
      "Epoch [141/200], Loss: 3123.8484, Learning Rate: 0.004575\n",
      "Epoch [142/200], Loss: 3123.7183, Learning Rate: 0.004575\n",
      "Epoch [143/200], Loss: 3124.4211, Learning Rate: 0.004575\n",
      "Epoch [144/200], Loss: 3124.7715, Learning Rate: 0.004575\n",
      "Epoch [145/200], Loss: 3123.3035, Learning Rate: 0.004575\n",
      "Epoch [146/200], Loss: 3123.1086, Learning Rate: 0.004575\n",
      "Epoch [147/200], Loss: 3123.2900, Learning Rate: 0.004575\n",
      "Epoch [148/200], Loss: 3122.9453, Learning Rate: 0.004575\n",
      "Epoch [149/200], Loss: 3122.8860, Learning Rate: 0.004575\n",
      "Epoch [150/200], Loss: 3122.7144, Learning Rate: 0.004118\n",
      "Epoch [151/200], Loss: 3122.6169, Learning Rate: 0.004118\n",
      "Epoch [152/200], Loss: 3122.5266, Learning Rate: 0.004118\n",
      "Epoch [153/200], Loss: 3123.0935, Learning Rate: 0.004118\n",
      "Epoch [154/200], Loss: 3122.7158, Learning Rate: 0.004118\n",
      "Epoch [155/200], Loss: 3122.3831, Learning Rate: 0.004118\n",
      "Epoch [156/200], Loss: 3122.3513, Learning Rate: 0.004118\n",
      "Epoch [157/200], Loss: 3122.5012, Learning Rate: 0.004118\n",
      "Epoch [158/200], Loss: 3123.2910, Learning Rate: 0.004118\n",
      "Epoch [159/200], Loss: 3122.6274, Learning Rate: 0.004118\n",
      "Epoch [160/200], Loss: 3123.6707, Learning Rate: 0.003706\n",
      "Epoch [161/200], Loss: 3122.2048, Learning Rate: 0.003706\n",
      "Epoch [162/200], Loss: 3122.1521, Learning Rate: 0.003706\n",
      "Epoch [163/200], Loss: 3122.2598, Learning Rate: 0.003706\n",
      "Epoch [164/200], Loss: 3122.1831, Learning Rate: 0.003706\n",
      "Epoch [165/200], Loss: 3122.6953, Learning Rate: 0.003706\n",
      "Epoch [166/200], Loss: 3122.3318, Learning Rate: 0.003706\n",
      "Epoch [167/200], Loss: 3122.1082, Learning Rate: 0.003706\n",
      "Epoch [168/200], Loss: 3122.2668, Learning Rate: 0.003706\n",
      "Epoch [169/200], Loss: 3122.3459, Learning Rate: 0.003706\n",
      "Epoch [170/200], Loss: 3122.8796, Learning Rate: 0.003335\n",
      "Epoch [171/200], Loss: 3121.8462, Learning Rate: 0.003335\n",
      "Epoch [172/200], Loss: 3121.7859, Learning Rate: 0.003335\n",
      "Epoch [173/200], Loss: 3121.8613, Learning Rate: 0.003335\n",
      "Epoch [174/200], Loss: 3122.0339, Learning Rate: 0.003335\n",
      "Epoch [175/200], Loss: 3121.8711, Learning Rate: 0.003335\n",
      "Epoch [176/200], Loss: 3121.8105, Learning Rate: 0.003335\n",
      "Epoch [177/200], Loss: 3121.6589, Learning Rate: 0.003335\n",
      "Epoch [178/200], Loss: 3122.4539, Learning Rate: 0.003335\n",
      "Epoch [179/200], Loss: 3121.8442, Learning Rate: 0.003335\n",
      "Epoch [180/200], Loss: 3121.7974, Learning Rate: 0.003002\n",
      "Epoch [181/200], Loss: 3121.4856, Learning Rate: 0.003002\n",
      "Epoch [182/200], Loss: 3121.4822, Learning Rate: 0.003002\n",
      "Epoch [183/200], Loss: 3121.8909, Learning Rate: 0.003002\n",
      "Epoch [184/200], Loss: 3121.6353, Learning Rate: 0.003002\n",
      "Epoch [185/200], Loss: 3121.6099, Learning Rate: 0.003002\n",
      "Epoch [186/200], Loss: 3121.4783, Learning Rate: 0.003002\n",
      "Epoch [187/200], Loss: 3121.4197, Learning Rate: 0.003002\n",
      "Epoch [188/200], Loss: 3121.3230, Learning Rate: 0.003002\n",
      "Epoch [189/200], Loss: 3121.4324, Learning Rate: 0.003002\n",
      "Epoch [190/200], Loss: 3121.3706, Learning Rate: 0.002702\n",
      "Epoch [191/200], Loss: 3121.1160, Learning Rate: 0.002702\n",
      "Epoch [192/200], Loss: 3121.5320, Learning Rate: 0.002702\n",
      "Epoch [193/200], Loss: 3121.0115, Learning Rate: 0.002702\n",
      "Epoch [194/200], Loss: 3121.2434, Learning Rate: 0.002702\n",
      "Epoch [195/200], Loss: 3122.1235, Learning Rate: 0.002702\n",
      "Epoch [196/200], Loss: 3121.1277, Learning Rate: 0.002702\n",
      "Epoch [197/200], Loss: 3121.0046, Learning Rate: 0.002702\n",
      "Epoch [198/200], Loss: 3121.1038, Learning Rate: 0.002702\n",
      "Epoch [199/200], Loss: 3121.1226, Learning Rate: 0.002702\n",
      "Epoch [200/200], Loss: 3120.8672, Learning Rate: 0.002432\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANN Model Construction\n",
    "class CustomANN(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(CustomANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, n, bias=True)  # input layer to hidden layer\n",
    "        self.fc2 = nn.Linear(n, n, bias=True)  # hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(n, n, bias=True)  # hidden layer 2 to hidden layer 3\n",
    "        self.fc4 = nn.Linear(n, 4)  # hidden layer 3 to output layer\n",
    "        self.activation = nn.LeakyReLU() # use PReLU as activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Model Initialization\n",
    "model = CustomANN(8) # create an instance\n",
    "model.to(device) # deploy model to cuda\n",
    "criterion = nn.MSELoss() # define loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.02) # define optimizer\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.9)  # learning rate schedule\n",
    "\n",
    "# Initialize an empty list to store the losses\n",
    "losses = []\n",
    "\n",
    "# Model Training\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []  # losses for the current epoch\n",
    "    for batch_position, batch_angle in dataloader:\n",
    "        # Forward Propagation\n",
    "        outputs = model(input_data)\n",
    "        loss = criterion(outputs, output_data)\n",
    "\n",
    "        # Backward Propagation and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses.append(average_epoch_loss)  # append the average loss to the list\n",
    "        \n",
    "    # Learning Rate Scheduler \n",
    "    scheduler.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "# Plot the loss\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Model Saving\n",
    "torch.save(model.state_dict(), 'model/custom_test_4_ann_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired Output: [ 90  90 -90 -90]; \n",
      "Model Output: [55.49876022338867, 59.786376953125, -7.9626545906066895, -1.8714256286621094]\n",
      "\n",
      "\n",
      "Desired Output: [  0   0   0   0]; \n",
      "Model Output: [-12.018527030944824, -0.20717430114746094, 0.6412577629089355, 44.503536224365234]\n"
     ]
    }
   ],
   "source": [
    "model = CustomANN(8).to(device)\n",
    "model_path = 'model/custom_test_4_ann_model.pth'\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "# Set the Model in Evaluation Mode\n",
    "model.eval()\n",
    "\n",
    "# Test1: the result for this test should be [90, 90, -90, -90]\n",
    "test1 = torch.tensor(\n",
    "    [1, 196.9859317102744, 298.4788975654116, 292.4788975654116], \n",
    "    dtype=torch.float32).to(device)  \n",
    "test2 = torch.tensor(\n",
    "    [1, 0, 0, 624],\n",
    "    dtype=torch.float32).to(device)\n",
    "\n",
    "# Perform Inference\n",
    "with torch.no_grad():\n",
    "    output1 = model(test1)\n",
    "    output2 = model(test2)\n",
    "\n",
    "# Print the Test Result\n",
    "print(f\"Desired Output: [ 90  90 -90 -90]; \\nModel Output: {output1.tolist()}\\n\\n\")\n",
    "print(f\"Desired Output: [  0   0   0   0]; \\nModel Output: {output2.tolist()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manipulator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
